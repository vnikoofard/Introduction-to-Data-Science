{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='https://sites.google.com/fat.uerj.br/livia'> <img src='../figures/capa2.png' /></a>\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maquinas de Suporte Verorial\n",
    "\n",
    "(baseado no excelente livro \"An Introduction to Statistical Learning with Applications in R\" de Gareth James, Daniela Witten, Trevor Hastie e Robert Tibshirani)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A máquina de vetores de suporte é uma generalização de um classificador simples e intuitivo chamado *classificador de margem máxima*, que apresentamos na primeira Seção. Embora seja elegante e simples, veremos que esse classificador infelizmente não pode ser aplicado à maioria dos conjuntos de dados, pois exige que as classes sejam separáveis por um limite linear. Na segunda Seção, apresentamos o classificador de vetor de suporte, uma extensão do classificador de margem máxima que pode ser aplicado em uma gama mais ampla de casos. A Seção três apresenta a máquina de vetores de suporte, que é uma extensão adicional do classificador de vetores de suporte para acomodar limites de classes não lineares.\n",
    "\n",
    "## Classificador de margem máxima\n",
    "\n",
    "O classificador de margem máxima é um classificador linear que tenta encontrar um hiperplano que divide as classes de forma que a distância entre o hiperplano e os pontos de treinamento mais próximos de cada classe seja maximizada. Esse classificador é simples e intuitivo, mas infelizmente não pode ser aplicado à maioria dos conjuntos de dados, pois exige que as classes sejam separáveis por um limite linear.\n",
    "\n",
    "### O que é um hiperplano?\n",
    "\n",
    "Em um espaço $p$-dimensional, um hiperplano é um subespaço afim plano de dimensão $p − 1$. Por exemplo, em duas dimensões, um hiperplano é um subespaço unidimensional plano - em outras palavras, uma linha. Em três dimensões, um hiperplano é um subespaço bidimensional plano, isto é, um plano. Em $p > 3$ dimensões, pode ser difícil visualizar um hiperplano, mas a noção de um subespaço plano $(p − 1)$ dimensional ainda se aplica.\n",
    "\n",
    "A definição matemática de um hiperplano é bastante simples. Em duas dimensões, um hiperplano é definido pela equação\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0$$\n",
    "\n",
    "onde $\\beta_0, \\beta_1, \\beta_2$ são constantes. Quando dizemos que essa equação “define” o hiperplano, queremos dizer que qualquer $X = (X_1 , X_2)^T$ para o qual a equação vale é um ponto no hiperplano. Observe que essa equação é simplesmente a equação de uma reta, pois de fato em duas dimensões um hiperplano é uma reta. A equação acima pode ser facilmente estendida para a configuração $p$-dimensional:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p = 0$$\n",
    "\n",
    "onde $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p$ são constantes. Quando dizemos que essa equação “define” o hiperplano, queremos dizer que qualquer $X = (X_1 , X_2, \\ldots, X_p)^T$ para o qual a equação vale é um ponto no hiperplano. \n",
    "\n",
    "Agora, suponha que $X$ não satisfaça a equação de hepirplano; ou seja,\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p > 0$$\n",
    "\n",
    "Então isso nos diz que $X$ está em um lado do hiperplano. Por outro lado, se\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p < 0$$\n",
    "\n",
    "então $X$ está do outro lado do hiperplano. Assim, podemos pensar no hiperplano como dividindo o espaço $p$-dimensional em duas metades. Pode-se facilmente determinar em que lado do hiperplano está um ponto simplesmente calculando o sinal do lado esquerdo da equação. Um hiperplano no espaço bidimensional é mostrado na figura abaixo.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15.hiper.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\"></figcaption>\n",
    "</figure>\n",
    "\n",
    "### Classificação usando um Hiperplano Separador\n",
    "\n",
    "Agora suponha que temos uma matriz de dados n×p X que consiste em n treinamento\n",
    "observações no espaço p-dimensional,\n",
    "\n",
    "$$\n",
    "x_1 = \n",
    "\\begin{pmatrix}\n",
    "x_{11}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_{1p}\n",
    "\\end{pmatrix}, \\ldots, x_n =\n",
    "\\begin{pmatrix}\n",
    "x_{n1}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_{np}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "e que essas observações se enquadram em duas classes - isto é, $y_1 , \\ldots , y_n \\in\n",
    "{−1, 1}$ onde $−1$ representa uma classe e $1$ a outra classe. Também temos uma observação de teste, um vetor $p$ de características observadas $x^* = (x^∗_1 \\dots x^∗_p)^T$. Nosso objetivo é desenvolver um classificador com base nos dados de treinamento que irá classificar corretamente a observação de teste usando suas medidas de características. Veremos agora uma nova abordagem baseada no conceito de hiperplano separador.\n",
    "\n",
    "Suponha que seja possível construir um hiperplano que separe perfeitamente as observações de treinamento do hiperplano de acordo com seus rótulos de classe. Exemplos de três desses hiperplanos separados são mostrados no painel esquerdo da figura abaixo. \n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-max.png\" style=\"width:70%\">\n",
    "<figcaption align = \"center\">Esquerda: Existem duas classes de observações, mostradas em azul e em roxo, cada uma com medições em duas variáveis. Três hiperplanos separados, dentre muitos possíveis, são mostrados em preto. Direita: Um hiperplano de separação é mostrado em preto. A grade azul e roxa indica a regra de decisão tomada por um classificador com base nesse hiperplano separador: uma observação de teste que caia na parte azul da grade será atribuída à classe azul e uma observação de teste que caia na parte roxa da gradeserá atribuída à classe roxa.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Podemos rotular as observações da classe azul como $y_i = 1$ e as da classe roxa como $y_i = −1$. Então um hiperplano separador tem a propriedade de que\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}> 0 \\;\\; \\text{se } \\;\\;y_i=1$$\n",
    "\n",
    "e\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}< 0 \\;\\; \\text{se } \\;\\;y_i=−1$$\n",
    "\n",
    "De forma equivalente, um hiperplano separador tem a propriedade de\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip})> 0$$\n",
    "\n",
    "\n",
    "para cada $i = 1, \\ldots, n$.\n",
    "\n",
    "Se existir um hiperplano de separação, podemos usá-lo para construir um classificador muito natural: uma observação de teste recebe uma classe dependendo de qual lado do hiperplano ela está localizada. O painel à direita da figura acima mostra um exemplo desse classificador. Ou seja, classificamos a observação de teste $x^∗$\n",
    "com base no sinal de $f (x^∗) = β_0 +β_1 x^∗_1 +β _2 x^∗_2 + \\ldots+ β_p x^∗_p$ . Se $f (x^∗)$ for positivo, então atribuímos a observação de teste à classe $1$, e se $f (x^∗)$ for negativo, então atribuímos à classe $−1$. Podemos também fazer uso da magnitude de $f(x^∗)$. Se $f(x^∗)$ estiver longe de zero, isso significa que $x^∗$ está longe do hiperplano e, portanto, podemos ter certeza sobre nossa atribuição de classe para $x^∗$ . Por outro lado, se $f(x^∗)$ é próximo de zero, então $x^∗$ está localizado próximo ao hiperplano, e assim temos menos certeza sobre a atribuição de classe para $x^∗$ . Não surpreendentemente, e como vemos na figura acima, um classificador baseado em um hiperplano de separação leva a um limite de decisão linear.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O classificador de margem máxima\n",
    "\n",
    "Em geral, se nossos dados puderem ser perfeitamente separados usando um hiperplano, então existirá de fato um número infinito de tais hiperplanos. Isso ocorre porque um determinado hiperplano de separação geralmente pode ser deslocado um pouquinho para cima ou para baixo, ou girado, sem entrar em contato com nenhuma das observações.\n",
    "Três possíveis hiperplanos separados são mostrados no painel esquerdo da figura acima. Para construir um classificador baseado em um hiperplano separador, devemos ter uma maneira razoável de decidir qual dos infinitos possíveis hiperplanos de separação a serem usados.\n",
    "Uma escolha natural é o hiperplano de margem máxima (também conhecido como hiperplano de separação ideal), que é o hiperplano de separação que está mais distante das observações de treinamento. Ou seja, podemos calcular a distância (perpendicular) de cada observação de treinamento para um determinado hiperplano de separação; a menor tal distância é a distância mínima das observações ao hiperplano, e é conhecida como *margem*. O hiperplano de margem máxima é o hiperplano de separação para o qual a margem é maior, ou seja, é o hiperplano que tem a distância mínima mais distante para as observações de treinamento. Podemos então classificar uma observação de teste com base em qual lado do hiperplano de margem máxima ela se encontra. Isso é conhecido como classificador de margem máxima. Esperamos que um classificador que tenha uma grande margem nos dados de treinamento também tenha uma grande margem nos dados de teste e, portanto, classifique as observações do teste corretamente. Embora o classificador de margem máxima seja frequentemente bem-sucedido, ele também pode levar ao overfitting quando $p$ é grande.\n",
    "\n",
    "Se $β_0, β_1, \\ldots, β_p$ são os coeficientes do hiperplano de margem máxima, então o classificador de margem máxima classifica a observação de teste x ∗ com base no sinal de $f(x^∗) = β_0 + β_1 x^∗_1 + β_2 x^∗_2 + \\ldots + β_p x^∗_p$.\n",
    "A figura abaixo mostra o hiperplano de margem máxima no conjunto de dados da Figura 9.2. Comparando o painel da direita da figura acima com a figura abaixo, vemos que o hiperplano de margem máxima mostrado na figura abaixo de fato resulta em uma distância mínima maior entre as observações e o hiperplano de separação - isto é, uma margem maior. De certa forma, o hiperplano de margem máxima representa a linha média da “laje” mais larga que podemos inserir entre as duas classes.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-support.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\">Existem duas classes de observações, mostradas em azul e em roxo. O hiperplano de margem máxima é mostrado como uma linha sólida. A margem é a distância da linha sólida até qualquer uma das linhas tracejadas. Os dois pontos azuis e o ponto roxo que estão nas linhas tracejadas são os vetores de suporte, e a distância desses pontos ao hiperplano é indicada por setas. A grade roxa e azul indica a regra de decisão feita por um classificador com base neste hiperplano separador.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Examinando a figura acima, vemos que três observações de treinamento são equidistantes do hiperplano da margem máxima e estão ao longo das linhas tracejadas que indicam a largura da margem. Essas três observações são conhecidas como vetores de suporte, pois são vetores no espaço p-dimensional (na figura acima, $p = 2$) e “suportam” o hiperplano da margem máxima no sentido de que, se esses pontos forem ligeiramente movidos, a margem máxima o hiperplano também se moveria. Curiosamente, o hiperplano de margem máxima depende diretamente dos vetores de suporte, mas não das outras observações: um movimento para qualquer uma das outras observações não afetaria o hiperplano de separação, desde que o movimento da observação não o faça cruzar o limite definido por A margem. O fato de que o hiperplano de margem máxima depende diretamente apenas de um pequeno subconjunto das observações é uma propriedade importante que surgirá mais adiante neste capítulo, quando discutirmos o classificador de vetores de suporte e as máquinas de vetores de suporte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção do Classificador de Margem Máxima\n",
    "\n",
    "Consideramos agora a tarefa de construir o hiperplano de margem máxima com base em um conjunto de $n$ observações de treinamento $x_1 , \\ldots, x_n \\in R^p$ e rótulos de classe associados $y_1 , \\ldots , y_n \\in \\{−1, 1\\}$. Resumidamente, o hiperplano de margem máxima é a solução para o problema de otimização\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max_{\\beta_0, \\beta_1,\\ldots,\\beta_p,M} M\\\\\n",
    "&\\text{sujeito a } \\sum^p_{j=1}\\beta^2_j = 1\\\\\n",
    "&y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M, \\quad i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Este problema de otimização é na verdade mais simples do que parece. Em primeiro lugar, a ultima restrição que, \n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M, \\quad i = 1, \\ldots, n$$\n",
    "\n",
    "garante que cada observação estará no lado correto do hiperplano, desde que $M$ seja positivo. (Na verdade, para cada observação estar no lado correto do hiperplano, precisaríamos simplesmente de $y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq 0$, então a ultima restrição de fato requer que cada observação esteja no lado correto do hiperplano, com alguma almofada, desde que $M$ seja positivo.)\n",
    "\n",
    "Em segundo lugar, observe que a segunda restrição não é realmente uma restrição no hiperplano, pois se $\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}= 0$ define um hiperplano, então $k(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) = 0$ para qualquer $k \\neq 0$. No entanto, a segunda restrição acrescenta significado a ultima; pode-se mostrar que com esta restrição a distância perpendicular da $i$-ésima observação ao hiperplano é dada por\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip})$$\n",
    "\n",
    "Portanto, as duas ultimas restrições garantem que cada observação esteja no lado correto do hiperplano e a pelo menos uma distância $M$ do hiperplano. Assim, $M$ representa a margem do nosso hiperplano, e o problema de otimização escolhe $β_0, β_1 , \\ldots, β_p$ para maximizar $M$ . Esta é exatamente a definição do hiperplano de margem máxima!\n",
    "\n",
    "### O Caso Inseparável\n",
    "\n",
    "O classificador de margem máxima é uma maneira muito natural de realizar a classificação, se existir um hiperplano de separação. No entanto, como sugerimos, em muitos casos não existe nenhum hiperplano de separação e, portanto, não há máxima classificador de margem. Neste caso, o problema de otimização acima não tem solução com $M > 0$. Um exemplo é mostrado na abaixo. Nesse caso, não podemos separar exatamente as duas classes. No entanto, como veremos na próxima seção, podemos estender o conceito de hiperplano separador para desenvolver um hiperplano que quase separe as classes, usando a chamada margem suave. A generalização do classificador de margem máxima para o caso não separável é conhecida como *classificador de vetor de suporte*.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-soft.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\">Existem duas classes de observações, mostradas em azul e em roxo. Nesse caso, as duas classes não são separáveis por um hiperplano e, portanto, o classificador de margem máxima não pode ser usado.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Classificadores de vetores de suporte\n",
    "\n",
    "### Visão geral do classificador de vetores de suporte\n",
    "\n",
    "Na figura acima, vemos que as observações que pertencem a duas classes não são necessariamente separáveis por um hiperplano. De fato, mesmo que exista um hiperplano de separação, há casos em que um classificador baseado em um hiperplano de separação pode não ser desejável. Um classificador baseado em um hiperplano separador necessariamente classificará perfeitamente todas as observações de treinamento; isso pode levar à sensibilidade a observações individuais. Um exemplo é mostrado na figura abaixo. A adição de uma única observação no painel direito da figura abaixo leva a uma mudança dramática no hiperplano da margem máxima. O hiperplano de margem máxima resultante não é satisfatório - por um lado, ele tem apenas uma pequena margem. Isso é problemático porque, conforme discutido anteriormente, a distância de uma observação do hiperplano pode ser vista como uma medida de nossa confiança de que a observação foi classificada corretamente. Além disso, o fato de o hiperplano de margem máxima ser extremamente sensível a uma mudança em uma única observação sugere que ele pode ter superajustado os dados de treinamento.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-out.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\">Esquerda: Duas classes de observações são mostradas em azul e roxo, juntamente com o hiperplano de margem máxima. Direita: Uma observação azul adicional foi adicionada, levando a uma mudança dramática no hiperplano da margem máxima mostrado como uma linha sólida. A linha tracejada indica o hiperplano de margem máxima que foi obtido na ausência desse ponto adicional.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Neste caso, podemos estar dispostos a considerar um classificador baseado em um hiperplano que não separe perfeitamente as duas classes, no interesse de\n",
    "\n",
    "- Maior robustez para observações individuais, e\n",
    "- Melhor classificação da maioria das observações de treinamento.\n",
    "\n",
    "Ou seja, pode valer a pena classificar incorretamente algumas observações de treinamento para fazer um trabalho melhor na classificação das observações restantes.\n",
    "\n",
    "O *classificador de vetor de suporte*, às vezes chamado de classificador de *margem flexível*, faz exatamente isso. Em vez de buscar a maior margem possível para que cada observação não esteja apenas no lado correto do hiperplano, mas também no lado correto da margem, permitimos que algumas observações estejam no lado incorreto da margem, ou mesmo no lado incorreto do hiperplano. (A margem é *suave* porque pode ser violada por algumas das observações de treinamento.) Um exemplo é mostrado no painel esquerdo da figura abaixo\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-soft2.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\">Esquerda: Um classificador de vetor de suporte foi ajustado a um pequeno conjunto de dados. O hiperplano é mostrado como uma linha sólida e as margens são mostradas como linhas tracejadas. Observações roxas: as observações 3, 4, 5 e 6 estão no lado correto da margem, a observação 2 está na margem e a observação 1 está no lado errado da margem. Observações azuis: as observações 7 e 10 estão no lado correto da margem, a observação 9 está na margem e a observação 8 está no lado errado da margem. Nenhuma observação está no lado errado do hiperplano. Direita: Igual ao painel esquerdo com dois pontos adicionais, 11 e 12. Essas duas observações estão no lado errado do hiperplano e no lado errado da margem.</figcaption>\n",
    "</figure>\n",
    "\n",
    "A maioria das observações estão no lado correto da margem.\n",
    "No entanto, um pequeno subconjunto das observações está do lado errado da margem.\n",
    "\n",
    "Uma observação pode estar não apenas no lado errado da margem, mas também no lado errado do hiperplano. De fato, quando não há hiperplano de separação, tal situação é inevitável. Observações no lado errado do hiperplano correspondem a observações de treinamento que são mal classificadas pelo classificador de vetores de suporte. O painel à direita da Figura 9.6 ilustra esse cenário."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalhes do Classificador de Vetores de Suporte\n",
    "O classificador de vetor de suporte classifica uma observação de teste dependendo de qual lado de um hiperplano ela se encontra. O hiperplano é escolhido para separar corretamente a maioria das observações de treinamento nas duas classes, mas pode classificar incorretamente algumas observações. É a solução do problema de otimização\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max_{\\beta_0, \\beta_1,\\ldots,\\beta_p, \\epsilon_1, \\ldots, \\epsilon_n, M} M\\\\\n",
    "&\\text{sujeito a } \\sum^p_{j=1}\\beta^2_j = 1\\\\\n",
    "&y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M (1-\\epsilon_i), \\\\\n",
    "&\\epsilon_i \\geq 0, \\quad \\sum^n_{i=1}\\epsilon_i \\leq C\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "onde $C$ é um parâmetro de ajuste não negativo. $M$ é a largura da margem; procuramos fazer esta quantidade tão grande quanto possível. As $\\epsilon_1, \\ldots, \\epsilon_n$ são variáveis de folga que permitem que observações individuais estejam no lado errado da margem ou do hiperplano; vamos explicá-los em maiores detalhes momentaneamente. Depois de resolvermos esse problema de otimização, classificamos uma observação de teste $x^∗$ como antes, simplesmente determinando de que lado do hiperplano ela se encontra. Ou seja, classificamos a observação do teste com base no sinal de $f (x^∗) = β_0 + β_1 x^∗_1 + \\ldots + β_p x^∗_p$.\n",
    "\n",
    "Este problema parece complexo, mas uma compreensão de seu comportamento pode ser feita por meio de uma série de observações simples apresentadas a seguir. Em primeiro lugar, a variável de folga $i$ nos diz onde a $i$-ésima observação está localizada, em relação ao hiperplano e em relação à margem. Se $i = 0$ então a $i$-ésima observação está no lado correto da margem. Se $i > 0$, então a $i$-ésima observação está do lado errado da margem, e dizemos que a $i$-ésima observação violou a margem. Se $i > 1$ então está no lado errado do hiperplano.\n",
    "Consideramos agora o papel do parâmetro de ajuste $C$. O hiperparametro $C$ limita a soma dos $i$'s, e assim determina o número e a severidade das violações à margem (e ao hiperplano) que iremos tolerar. Podemos pensar em $C$ como um *orçamento* para o valor que a margem pode ser violada pelas $n$ observações. Se $C = 0$, então não há orçamento para violações à margem, e deve ser o caso de $\\epsilon_1 = \\ldots = \\epsilon_n = 0$, neste caso o problema simplesmente equivale à margem máxima problema de otimização de hiperplano. (Claro, um hiperplano de margem máxima só existe se as duas classes forem separáveis.) Para $C > 0$, não mais que $C$ observações podem estar no lado errado do hiperplano, porque se uma observação estiver no lado errado do hiperplano então $\\epsilon_i > 1$, e a ultima restrição requer que $\\sum^n_{i=1} \\epsilon_i \\leq C$. À medida que o orçamento $C$ aumenta, nos tornamos mais tolerantes a violações da margem e, portanto, a margem aumentará. Por outro lado, à medida que $C$ diminui, nos tornamos menos tolerantes a violações da margem e, portanto, a margem se estreita. Um exemplo é mostrado na Figura abaixo.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-machine.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\">Um classificador de vetor de suporte foi ajustado usando quatro valores diferentes do parâmetro de ajuste C. O maior valor de C foi usado no painel superior esquerdo e valores menores foram usados nos painéis superior direito, inferior esquerdo e inferior direito. Quando C é grande, então há uma alta tolerância para as observações estarem no lado errado da margem e, portanto, a margem será grande. À medida que C diminui, a tolerância para observações do lado errado da margem diminui e a margem se estreita.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Na prática, $C$ é tratado como um parâmetro de ajuste que geralmente é escolhido por validação cruzada. Assim como os parâmetros de ajuste que vimos ao longo deste livro, $C$ controla a compensação de viés-variância da técnica de aprendizado estatístico. Quando $C$ é pequeno, buscamos margens estreitas que raramente são violadas; isso equivale a um classificador altamente adequado aos dados, que pode ter um viés baixo, mas uma variância alta. Por outro lado, quando $C$ é maior, a margem é maior e permitimos mais violações a ela; isso equivale a ajustar os dados com menos dificuldade e obter um classificador que é potencialmente mais tendencioso, mas pode ter menor variância.\n",
    "\n",
    "O problema de otimização acima tem uma propriedade muito interessante: verifica-se que apenas as observações que estão na margem ou que violam a margem afetarão o hiperplano e, portanto, o classificador obtido. Em outras palavras, uma observação que está estritamente no lado correto da margem não afeta o classificador de vetor de suporte! Mudar a posição dessa observação não mudaria em nada o classificador, desde que sua posição permaneça no lado correto da margem. Observações que estão diretamente na margem, ou no lado errado da margem de sua classe, são conhecidas como vetores de suporte. Essas observações afetam o classificador do vetor de suporte.\n",
    "\n",
    "O fato de que apenas os vetores de suporte afetam o classificador está de acordo com nossa afirmação anterior de que $C$ controla a compensação de viés-variância do classificador de vetores de suporte. Quando o parâmetro de ajuste $C$ é grande, a margem é ampla, muitas observações violam a margem e, portanto, há muitos vetores de suporte. Neste caso, muitas observações estão envolvidas na determinação do hiperplano. O painel superior esquerdo na figura acima ilustra essa configuração: esse classificador tem baixa variância (já que muitas observações são vetores de suporte), mas um viés potencialmente alto. Em contraste, se $C$ for pequeno, haverá menos vetores de suporte e, portanto, o classificador resultante terá baixo viés, mas alta variância. O painel inferior direito na figura acima ilustra essa configuração, com apenas oito vetores de suporte.\n",
    "\n",
    "O fato de a regra de decisão do classificador de vetores de suporte ser baseada apenas em um subconjunto potencialmente pequeno das observações de treinamento (os vetores de suporte) significa que ela é bastante robusta ao comportamento de observações distantes do hiperplano. Essa propriedade é distinta de alguns dos outros métodos de classificação que vimos nos capítulos anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Máquinas de Vetores de Suporte\n",
    "\n",
    "Primeiro discutimos um mecanismo geral para converter um classificador linear em um que produz limites de decisão não lineares. Apresentamos então a máquina de vetores de suporte, que faz isso de forma automática.\n",
    "\n",
    "### Classificação com Limites de Decisão Não Lineares\n",
    "\n",
    "O classificador de vetor de suporte é uma abordagem natural para classificação na configuração de duas classes, se o limite entre as duas classes for linear. No entanto, na prática, às vezes nos deparamos com limites de classe não lineares. Por exemplo, considere os dados no painel esquerdo da figura abaixo. É claro que um classificador de vetor de suporte ou qualquer classificador linear terá um desempenho ruim aqui. De fato, o classificador de vetores de suporte mostrado no painel direito da figura abaixo é inútil aqui.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-nlinear.png\" style=\"width:70%\">\n",
    "<figcaption align = \"center\">Esquerda: As observações se enquadram em duas classes, com um limite não linear entre elas. Direita: O classificador de vetor de suporte busca um limite linear e, conseqüentemente, executa muito mal.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Nos capitulos anteriores, nos deparamos com uma situação análoga. vemos lá\n",
    "que o desempenho da regressão linear pode sofrer quando há uma relação não linear entre os preditores e o resultado. Nesse caso, consideramos ampliar o espaço de features usando funções dos preditores, como termos quadráticos e cúbicos, a fim de resolver essa não linearidade. No caso do classificador de vetor de suporte, poderíamos abordar o problema de limites possivelmente não lineares entre as classes de maneira semelhante, ampliando o espaço de features usando funções quadráticas, cúbicas e até mesmo polinomiais de ordem superior dos preditores. Por exemplo, em vez de ajustar um classificador de vetor de suporte usando $p$ features\n",
    "\n",
    "$$X_1, X_2, \\ldots, X_p$$\n",
    "\n",
    "em vez disso, poderíamos ajustar um classificador de vetor de suporte usando $2p$ features\n",
    "\n",
    "$$X_1, X_1^2, X_2, X_2^2\\ldots, X_p, X_p^2$$\n",
    "\n",
    "Nesta situação o problema de otimização se torna\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max_{\\beta_0, \\beta_1,\\ldots,\\beta_p, \\epsilon_1, \\ldots, \\epsilon_n, M} M\\\\\n",
    "&\\text{sujeito a } \\sum^p_{j=1} \\sum^2_{k=1}\\beta^2_{jk} = 1\\\\\n",
    "&y_i(\\beta_0 + \\sum_{j=1}^p \\beta_{j1}x_{ij} + \\sum_{j=1}^p \\beta_{j2}x^2_{ij})  \\geq M (1-\\epsilon_i), \\\\\n",
    "&\\epsilon_i \\geq 0, \\quad \\sum^n_{i=1}\\epsilon_i \\leq C\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Por que isso leva a um limite de decisão não linear? No espaço de features ampliado, o limite de decisão que resulta da equação acima é de fato linear. Mas no espaço de features original, o limite de decisão é da forma $q(x) = 0$, onde $q$ é um polinômio quadrático, e suas soluções são geralmente não lineares. Além disso, pode-se querer ampliar o espaço de recursos com termos polinomiais de ordem superior ou com termos de interação da forma $X_j X_{j'}$ para $j \\neq j'$ . Alternativamente, outras funções dos preditores poderiam ser consideradas em vez de polinômios. Não é difícil ver que existem muitas maneiras possíveis de aumentar o espaço de features e que, a menos que tenhamos cuidado, podemos acabar com um grande número de features. Então os cálculos se tornariam incontroláveis. A máquina de vetores de suporte, que apresentamos a seguir, nos permite ampliar o espaço de features usado pelo classificador de vetores de suporte de uma forma que leva a cálculos eficientes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Máquina de Vetores de Suporte\n",
    "\n",
    "A máquina de vetores de suporte (SVM) é uma extensão do classificador de vetores de suporte que resulta da ampliação do espaço de features de uma maneira específica, usando *kernels*. Discutiremos agora essa extensão, cujos detalhes são um tanto complexos e estão além do escopo deste livro. No entanto, a ideia principal é descrita na Seção anterior: podemos querer ampliar nosso espaço de features para acomodar uma fronteira não linear entre as classes. A abordagem do kernel que descrevemos aqui é simplesmente uma abordagem computacional eficiente para implementar essa ideia.\n",
    "\n",
    "Não discutimos exatamente como o classificador de vetor de suporte é calculado porque os detalhes se tornam um tanto técnicos. No entanto, verifica-se que a solução para o problema do classificador de vetores de suporte envolve apenas os produtos internos das observações (em oposição às próprias observações). O produto interno de dois vetores $r$-dimensionais $\\vec a$ e $\\vec b$ é definido como $<a,b> = \\sum^r_{i=1}a_i b_i$ . Assim, o produto interno de duas observações $x_i, x_{i'}$ são dadas por\n",
    "\n",
    "$$<x_i, x_{i'}> = \\sum^p_{i=1}x_{ij} x_{i'j}.$$\n",
    "\n",
    "Pode-se demontrado que \n",
    "\n",
    "- O classificador de vetor de suporte linear pode ser representado como\n",
    "\n",
    "$$f(x)=\\beta_0 + \\sum^n_{i=1} \\alpha_i <x, x_{i}>$$\n",
    "\n",
    "onde existem $n$ parâmetros $\\alpha_i$ , $i = 1,\\ldots, n$, um por observação de treinamento.\n",
    "\n",
    "- Para estimar os parametros $\\alpha_1, \\ldots, \\alpha_n$ and $\\beta_0$, precisamos calcular $\\begin{pmatrix} n\\\\2 \\end{pmatrix}$ produto interno $<x_i, x_{i'}>$ entre todos os pares das observações de treino. \n",
    "\n",
    "Observe que na equação acima, para calcular a função $f(x)$, precisamos calcular o produto interno entre o novo ponto $x$ e cada um dos pontos de treinamento $x_i$ . No entanto, verifica-se que $\\alpha_i$ é diferente de zero apenas para os vetores de suporte na solução – ou seja, se uma observação de treinamento não é um vetor de suporte, então seu $\\alpha_i$ é igual a zero. Portanto, se $\\mathcal S$ é a coleção de índices desses pontos de apoio, podemos reescrever qualquer função solução da forma da equação acima como\n",
    "\n",
    "$$f(x)=\\beta_0 + \\sum^n_{i=\\mathcal S} \\alpha_i <x, x_{i}>$$\n",
    "\n",
    "que normalmente envolve muito menos termos.\n",
    "\n",
    "Para resumir, ao representar o classificador linear $f(x)$ e ao calcular seus coeficientes, tudo o que precisamos são produtos internos.\n",
    "\n",
    "Agora suponha que toda vez que o produto interno em um cálculo da solução para o classificador de vetores de suporte, nós o substituímos por uma *generalização* do produto interno da forma\n",
    "\n",
    "$$K(x_i, x_{i'})$$\n",
    "\n",
    "onde $K$ é alguma função que chamaremos de kernel. Um kernel é uma função que quantifica a similaridade de duas observações. Por exemplo, poderíamos simplesmente tomar\n",
    "\n",
    "$$K(x_i, x_{i'}) = \\sum^p_{i=1}x_{ij} x_{i'j}$$\n",
    "\n",
    "o que apenas nos devolveria o classificador de vetores de suporte. A equação acima é conhecida como $kernel linear$ porque o classificador do vetor de suporte é linear nas feições; o kernel linear essencialmente quantifica a similaridade de um par de observações usando a correlação de Pearson (padrão). Mas pode-se escolher outra forma para um kernel. Por exemplo, pode-se substituir cada instância de $\\sum^p_{i=1}x_{ij} x_{i'j}$ com a quantidade\n",
    "\n",
    "$$K(x_i, x_{i'}) = (1 + \\sum^p_{i=1}x_{ij} x_{i'j})^d$$\n",
    "\n",
    "Isso é conhecido como $kernel polinomial$ de grau $d$, onde $d$ é um numero inteiro positivo. Usar tal kernel com $d > 1$, em vez do kernel do kernel linear padrão, no algoritmo do classificador de vetores de suporte leva a um limite de decisão muito mais flexível. Isso equivale essencialmente a ajustar um classificador de vetor de suporte em um espaço de dimensão superior envolvendo polinômios de grau $d$, em vez do espaço de features original. Quando o classificador de vetores de suporte é combinado com um kernel não linear, o classificador resultante é conhecido como uma $máquina de vetores de suporte$. Note que neste caso a função (não linear) tem a forma\n",
    "\n",
    "$$f(x)=\\beta_0 + \\sum^n_{i=\\mathcal S} \\alpha_i K(x, x_{i})$$\n",
    "\n",
    "O painel esquerdo da figura abaixo mostra um exemplo de SVM com um kernel polinomial aplicado aos dados não lineares da figura acima. O ajuste é uma melhoria substancial em relação ao classificador de vetor de suporte linear. Quando $d = 1$, o SVM se reduz ao classificador de vetor de suporte visto anteriormente neste capítulo.\n",
    "\n",
    "O kernel polinomial é um exemplo de um possível kernel não linear, mas há muitas alternativas. Outra escolha popular é o kernel radial, que assume a forma\n",
    "\n",
    "$$K(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum^p_{j=1}(x_{ij} - x_{i'j})^2 \\right)$$\n",
    "\n",
    "Nesta equação, $\\gamma$ é uma constante positiva. O painel direito da Figura abaixo mostra um exemplo de SVM com kernel radial nesses dados não lineares; ele também faz um bom trabalho em separar as duas classes.\n",
    "\n",
    "Como o kernel radial realmente funciona? Se uma dada observação de teste $x^* = (x^*_1, \\ldots, x^*_p)^T$ está longe de uma observação de treinamento $x_i$ em termos de distância Euclidiana, então $\\sum^p_{j=1}(x_{ij} - x_{i'j})^2$ será grande e, portanto, $K(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum^p_{j=1}(x_{ij} - x_{i'j})^2 \\right)$ será minúsculo. Isso significa que em , $x_i$ praticamente não terá nenhum papel em $f (x^∗)$. Lembre-se de que o rótulo de classe previsto para a observação de teste $x^∗$ é baseado no sinal de $f (x^∗)$. Em outras palavras, as observações de treinamento que estão longe de $x^∗$ não desempenharão praticamente nenhum papel no rótulo de classe previsto para $x^∗$.\n",
    "\n",
    "Isso significa que o kernel radial tem um comportamento muito *local*, no sentido de que apenas observações de treinamento próximas têm efeito no rótulo de classe de uma observação de teste.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../figures/15-n.png\" style=\"width:70%\">\n",
    "<figcaption align = \"center\">Esquerda: Um SVM com kernel polinomial de grau 3 é aplicado aos dados não lineares da figura anterior, resultando em uma regra de decisão muito mais apropriada. Direita: Um SVM com um kernel radial é aplicado. Neste exemplo, qualquer kernel é capaz de capturar o limite de decisão.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Qual é a vantagem de usar um kernel em vez de simplesmente aumentar o espaço de features usando funções dos features originais? Uma vantagem é computacional, e isso equivale ao fato de que, usando kernels, é necessário apenas calcular $K(x_i , x'_i)$ para todos os $\\begin{pmatrix} n\\\\2 \\end{pmatrix}$ pares distintos $i, i'$ . Isso pode ser feito sem trabalhar explicitamente no espaço de features ampliado. Isso é importante porque em muitas aplicações de SVMs, o espaço de features ampliado é tão grande que os cálculos são intratáveis. Para alguns kernels, como o kernel radial, o espaço de features é $implícito$ e de dimensão infinita, então nunca poderíamos fazer os cálculos lá de qualquer maneira!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Implementação do Maquinas de Suporte Verorial no Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Dataset\n",
    "\n",
    "\n",
    "\n",
    "Usaremos o conjunto de dados integrado de câncer de mama do Scikit Learn. Podemos obter com a função load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados é apresentado em forma de dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obter informações e matrizes deste dicionário para configurar nosso quadro de dados e entender os recursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['feature_names']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Dataset e Criando o Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "df_feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(cancer['target'],columns=['Cancer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos verificar o dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cancer\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.30, random_state=101)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar a Maquina de Vetores de Suporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões e Avaliações\n",
    "\n",
    "Agora vamos prever usando o modelo treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56  10]\n",
      " [  3 102]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90        66\n",
      "           1       0.91      0.97      0.94       105\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.93      0.91      0.92       171\n",
      "weighted avg       0.93      0.92      0.92       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uau! Observe que estamos classificando tudo em uma única classe! Isso significa que nosso modelo precisa ter seus parâmetros ajustados (isso também pode ajudar a normalizar os dados).\n",
    "\n",
    "Podemos procurar parâmetros usando um GridSearch!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "\n",
    "Encontrar os parâmetros corretos (como quais valores C ou gama usar) é uma tarefa complicada! Mas, felizmente, podemos ser um pouco preguiçosos e tentar várias combinações e ver o que funciona melhor! Essa ideia de criar uma 'grade' de parâmetros e apenas tentar todas as combinações possíveis é chamada de Gridsearch, esse método é comum o suficiente para que o Scikit-learn tenha essa funcionalidade incorporada ao GridSearchCV! O CV significa validação cruzada, que é o\n",
    "\n",
    "GridSearchCV pega um dicionário que descreve os parâmetros que devem ser testados e um modelo para treinar. A grade de parâmetros é definida como um dicionário, onde as chaves são os parâmetros e os valores são as configurações a serem testadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das grandes vantagens do GridSearchCV é que ele é um meta-estimador. Ele pega um estimador como o SVC e cria um novo estimador, que se comporta exatamente da mesma forma - neste caso, como um classificador. Você deve adicionar refit=True e escolher detalhado para qualquer número que desejar, quanto maior o número, mais detalhado (verbose significa apenas a saída de texto que descreve o processo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que o ajuste faz é um pouco mais complicado do que o normal. Primeiro, ele executa o mesmo loop com validação cruzada para encontrar a melhor combinação de parâmetros. Uma vez obtida a melhor combinação, ele executa o ajuste novamente em todos os dados passados para ajuste (sem validação cruzada), para construir um único novo modelo usando a melhor configuração de parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.938 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.963 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.886 total time=   0.0s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.950 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.950 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.949 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.949 total time=   0.0s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.949 total time=   0.0s\n",
      "[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.938 total time=   0.0s\n",
      "[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.963 total time=   0.0s\n",
      "[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.924 total time=   0.0s\n",
      "[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.962 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take awhile!\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode inspecionar os melhores parâmetros encontrados por GridSearchCV no atributo best_params_ e o melhor estimador no atributo best_estimator_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=0.0001)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você pode executar novamente as previsões nesse objeto de grade, exatamente como faria com um modelo normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59   7]\n",
      " [  4 101]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        66\n",
      "           1       0.94      0.96      0.95       105\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizando ...\n",
    "\n",
    "Qual é a diferença e (des)vantagens dos modelos baseado em Ensemle e SVM? De fato, métodos de ensemle e Máquinas de Vetores de Suporte (SVMs) são ambas técnicas populares em aprendizado de máquina, cada uma com seu próprio conjunto de vantagens e desvantagens. Vamos compará-las:\n",
    "\n",
    "### Métodos de Ensemle:\n",
    "\n",
    "#### Vantagens:\n",
    "1. **Melhor Generalização:**\n",
    "   - Métodos de ensemle, como Florestas Aleatórias e Gradiente Boosting, frequentemente levam a um desempenho de generalização melhor em comparação com modelos individuais. Eles podem capturar relações complexas nos dados.\n",
    "\n",
    "2. **Robustez:**\n",
    "   - Ensemles são robustos a valores atípicos e dados ruidosos. Outliers têm menos probabilidade de ter um impacto significativo no modelo geral, tornando os ensemles mais confiáveis.\n",
    "\n",
    "3. **Versatilidade:**\n",
    "   - Ensemle podem ser usados com vários aprendizes base, proporcionando flexibilidade para modelar diferentes tipos de relações nos dados.\n",
    "\n",
    "4. **Redução de Overfitting:**\n",
    "   - Métodos de ensemle, particularmente o bagging (por exemplo, Florestas Aleatórias), ajudam a reduzir o overfitting ao fazer a média ou combinar as previsões de vários modelos.\n",
    "\n",
    "#### Desvantagens:\n",
    "1. **Complexidade Computacional:**\n",
    "   - Treinar e fazer previsões com métodos de ensemle pode ser computacionalmente caro, especialmente à medida que o número de aprendizes base aumenta.\n",
    "\n",
    "2. **Interpretabilidade:**\n",
    "   - Conjuntos são frequentemente considerados modelos \"caixa preta\", tornando desafiador interpretar as relações aprendidas e fornecer insights sobre o processo de tomada de decisão.\n",
    "\n",
    "### Máquinas de Vetores de Suporte (SVMs):\n",
    "\n",
    "#### Vantagens:\n",
    "1. **Efetivas em Espaços de Alta Dimensão:**\n",
    "   - SVMs se saem bem em espaços de alta dimensão, sendo adequadas para problemas com um grande número de características.\n",
    "\n",
    "2. **Eficiência de Memória:**\n",
    "   - SVMs utilizam um subconjunto de pontos de treinamento (vetores de suporte) na função de decisão, tornando-as eficientes em termos de memória, especialmente em situações com recursos limitados.\n",
    "\n",
    "3. **Truque do Kernel:**\n",
    "   - O truque do kernel permite que SVMs mapeiem implicitamente os dados de entrada para espaços de dimensão superior, tornando-as capazes de capturar relações não lineares complexas.\n",
    "\n",
    "4. **Optimalidade Global:**\n",
    "   - SVMs buscam encontrar o hiperplano que separa as classes de maneira maximamente eficaz, e a solução frequentemente é global, garantindo uma otimização mais robusta.\n",
    "\n",
    "#### Desvantagens:\n",
    "1. **Sensibilidade à Sintonia de Parâmetros:**\n",
    "   - O desempenho das SVMs é sensível à escolha de hiperparâmetros, como o parâmetro de regularização (C) e os parâmetros do kernel. A sintonia fina desses parâmetros pode ser desafiadora.\n",
    "\n",
    "2. **Limitação de Escalabilidade:**\n",
    "   - SVMs podem se tornar menos eficientes e escaláveis à medida que o tamanho do conjunto de dados aumenta, especialmente para kernels não lineares.\n",
    "\n",
    "3. **Classificação Binária:**\n",
    "   - As SVMs são inherentemente projetadas para tarefas de classificação binária. Embora existam extensões para problemas de várias classes, elas podem não ter um desempenho tão natural quanto outros métodos projetados especificamente para cenários de várias classes.\n",
    "\n",
    "Em resumo, a escolha entre métodos de ensemle e SVMs depende das características específicas do problema em questão, do tamanho do conjunto de dados, dos requisitos de interpretabilidade e dos recursos computacionais disponíveis. Métodos de ensemle são frequentemente preferidos por suas fortes capacidades de generalização, enquanto as SVMs são eficazes em espaços de alta dimensão e em situações em que uma margem clara de separação é crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "92b1d5f842d7c1e714e1ab666cefb8d1f75311aac337aae0336133957fcbf554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
